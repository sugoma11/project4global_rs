{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import ResNet\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models.resnet import ResNet\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils.log import *\n",
    "from utils.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Wandb logging and setup from config  \n",
    "1.1 - Sort of builder from cfg\n",
    "2 - Dataloader with slicing  \n",
    "3 - Define some models  \n",
    "4 - Wrap it into slurm tasks and add saving of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock1x1(nn.Module):\n",
    "    \"\"\"Basic Resnet block but instead of 3x3 convs we use 1x1 convs\"\"\"\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 planes,\n",
    "                 stride = 1,\n",
    "                 downsample = None,\n",
    "                 groups = 1,\n",
    "                 base_width = 64,\n",
    "                 dilation = 1,\n",
    "                 norm_layer = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv1x1(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s1, y_train_s1, full_distrib_train_s1, X_val_s1, y_val_s1, full_distrib_val_s1 = load_pickled_ds('data/pickled_data/raw_train_test_splitted_s1_60.pkl')\n",
    "X_train_s2, y_train_s2, full_distrib_train_s2, X_val_s2, y_val_s2, full_distrib_val_s2 = load_pickled_ds('data/pickled_data/raw_train_test_splitted_s2_60.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_order_s2 = [\"B02\", \"B03\", \"B04\", \"B08\", \"B05\", \"B06\", \"B07\", \"B08A\", \"B11\", \"B12\", \"B01\", \"B09\"]\n",
    "channel_order_s1 = [\"VV\", \"VH\", \"VV-VH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "bs = 512\n",
    "\n",
    "ri = [\"NDVI\", \"EVI\", \"NDWI\", \"GNDVI\", \"SAVI\", \"ARVI\", \"MSAVI\"]\n",
    "\n",
    "train_dataset_s1 = NumpyDataset(X_train_s1, y_train_s1, sentinel_number=1, band_order=channel_order_s1, requested_indices=[\"VV\", \"VH\", \"VV-VH\"])\n",
    "train_dataset_s2 = NumpyDataset(X_train_s2, y_train_s2, sentinel_number=2, band_order=channel_order_s2,\n",
    "                                requested_indices=ri)\n",
    "\n",
    "\n",
    "val_dataset_s1 = NumpyDataset(X_val_s1, y_val_s1, sentinel_number=1, band_order=channel_order_s1, requested_indices=[\"VV\", \"VH\", \"VV-VH\"])\n",
    "val_dataset_s2 = NumpyDataset(X_val_s2, y_val_s2, sentinel_number=2, band_order=channel_order_s2, \n",
    "                              requested_indices=ri)\n",
    "\n",
    "\n",
    "train_dataloader_s1 = DataLoader(train_dataset_s1, batch_size=bs, shuffle=True, generator=g)\n",
    "train_dataloader_s2 = DataLoader(train_dataset_s2, batch_size=bs, shuffle=True, generator=g)\n",
    "\n",
    "val_dataloader_s1 = DataLoader(val_dataset_s1, batch_size=bs, shuffle=True, generator=g)\n",
    "val_dataloader_s2 = DataLoader(val_dataset_s2, batch_size=bs, shuffle=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'models.fc' from '/home/al/projects/global_rs_project/models/fc.py'> MLPSmall\n",
      "64 32\n",
      "Epoch [1/30], Loss: 2.6436, Accuracy: 12.81%\n",
      "Validation Loss: 23.0210, Accuracy: 17.61%\n",
      "Epoch [2/30], Loss: 2.5820, Accuracy: 16.64%\n",
      "Validation Loss: 22.6941, Accuracy: 17.68%\n",
      "Epoch [3/30], Loss: 2.5542, Accuracy: 17.36%\n",
      "Validation Loss: 22.4890, Accuracy: 18.25%\n",
      "Epoch [4/30], Loss: 2.5372, Accuracy: 18.43%\n",
      "Validation Loss: 22.3052, Accuracy: 19.32%\n",
      "Epoch [5/30], Loss: 2.5201, Accuracy: 18.96%\n",
      "Validation Loss: 22.2002, Accuracy: 20.10%\n",
      "Epoch [6/30], Loss: 2.5031, Accuracy: 19.64%\n",
      "Validation Loss: 21.4435, Accuracy: 24.71%\n",
      "Epoch [7/30], Loss: 2.3870, Accuracy: 24.48%\n",
      "Validation Loss: 20.6916, Accuracy: 26.14%\n",
      "Epoch [8/30], Loss: 2.3583, Accuracy: 25.12%\n",
      "Validation Loss: 20.5481, Accuracy: 26.40%\n",
      "Epoch [9/30], Loss: 2.3431, Accuracy: 25.19%\n",
      "Validation Loss: 20.4618, Accuracy: 26.52%\n",
      "Epoch [10/30], Loss: 2.3308, Accuracy: 25.60%\n",
      "Validation Loss: 20.4207, Accuracy: 26.54%\n",
      "Epoch [11/30], Loss: 2.3277, Accuracy: 25.68%\n",
      "Validation Loss: 20.3941, Accuracy: 26.80%\n",
      "Epoch [12/30], Loss: 2.3235, Accuracy: 25.91%\n",
      "Validation Loss: 20.3597, Accuracy: 26.72%\n",
      "Epoch [13/30], Loss: 2.3197, Accuracy: 26.19%\n",
      "Validation Loss: 20.3356, Accuracy: 26.80%\n",
      "Epoch [14/30], Loss: 2.3161, Accuracy: 26.16%\n",
      "Validation Loss: 20.3121, Accuracy: 26.96%\n",
      "Epoch [15/30], Loss: 2.3133, Accuracy: 26.17%\n",
      "Validation Loss: 20.2963, Accuracy: 26.89%\n",
      "Epoch [16/30], Loss: 2.3103, Accuracy: 26.25%\n",
      "Validation Loss: 20.2757, Accuracy: 27.02%\n",
      "Epoch [17/30], Loss: 2.3099, Accuracy: 26.14%\n",
      "Validation Loss: 20.2587, Accuracy: 27.06%\n",
      "Epoch [18/30], Loss: 2.3088, Accuracy: 26.31%\n",
      "Validation Loss: 20.2449, Accuracy: 26.87%\n",
      "Epoch [19/30], Loss: 2.3061, Accuracy: 26.32%\n",
      "Validation Loss: 20.2342, Accuracy: 27.04%\n",
      "Epoch [20/30], Loss: 2.3041, Accuracy: 26.37%\n",
      "Validation Loss: 20.2131, Accuracy: 27.01%\n",
      "Epoch [21/30], Loss: 2.3019, Accuracy: 26.42%\n",
      "Validation Loss: 20.2062, Accuracy: 27.11%\n",
      "Epoch [22/30], Loss: 2.3017, Accuracy: 26.46%\n",
      "Validation Loss: 20.2046, Accuracy: 27.07%\n",
      "Epoch [23/30], Loss: 2.3010, Accuracy: 26.51%\n",
      "Validation Loss: 20.1932, Accuracy: 27.24%\n",
      "Epoch [24/30], Loss: 2.2991, Accuracy: 26.53%\n",
      "Validation Loss: 20.1848, Accuracy: 27.17%\n",
      "Epoch [25/30], Loss: 2.2980, Accuracy: 26.55%\n",
      "Validation Loss: 20.1704, Accuracy: 27.14%\n",
      "Epoch [26/30], Loss: 2.2972, Accuracy: 26.58%\n",
      "Validation Loss: 20.1617, Accuracy: 27.25%\n",
      "Epoch [27/30], Loss: 2.2954, Accuracy: 26.73%\n",
      "Validation Loss: 20.1528, Accuracy: 27.24%\n",
      "Epoch [28/30], Loss: 2.2935, Accuracy: 26.51%\n",
      "Validation Loss: 20.1462, Accuracy: 27.31%\n",
      "Epoch [29/30], Loss: 2.2965, Accuracy: 26.63%\n",
      "Validation Loss: 20.1414, Accuracy: 27.29%\n",
      "Epoch [30/30], Loss: 2.2941, Accuracy: 26.77%\n",
      "Validation Loss: 20.1377, Accuracy: 27.29%\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from models import get_model\n",
    "\n",
    "ce = CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sample = next(iter(train_dataloader_s1))[0]\n",
    "n_bands = sample.shape[1]\n",
    "input_size = sample.shape[2]\n",
    "rs = get_model('fc.MLPSmall')\n",
    "rs = rs(n_classes=15, n_bands=n_bands, input_size=input_size)\n",
    "rs.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(rs.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "num_epochs = 30  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rs.train() \n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, _, labels in train_dataloader_s1:\n",
    "        inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = rs.forward(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader_s2):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation loop\n",
    "    rs.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient calculation during validation\n",
    "        for inputs, _, labels in train_dataloader_s1:\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = rs(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Statistics\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {val_loss/len(val_dataloader_s2):.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_loader, val_loader, lr, weight_decay, loss_fn, metric_fn,\n",
    "                       device, num_epochs, step2decay, decay_lr):\n",
    "    \n",
    "    \n",
    "    indiced_sample, raw_sample = next(iter(train_loader))[:2]\n",
    "    n_bands_raw, n_bands_indices = indiced_sample.shape[1], raw_sample.shape[1]\n",
    "\n",
    "    rs_indiced = Resnet1x1(n_classes=15, n_bands=n_bands_indices)\n",
    "    rs_raw = Resnet1x1(n_classes=15, n_bands=n_bands_raw)\n",
    "\n",
    "    rs_indiced.model.to(device)\n",
    "    rs_raw.model.to(device)\n",
    "\n",
    "    loss_fn_raw = loss_fn().to(device)\n",
    "    loss_fn_indiced = loss_fn().to(device)\n",
    "\n",
    "\n",
    "    optimizer_raw = optim.AdamW(rs_raw.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    optimizer_indiced = optim.AdamW(rs_indiced.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler_raw = StepLR(optimizer_raw, step_size=step2decay, gamma=decay_lr)\n",
    "    scheduler_indiced = StepLR(optimizer_indiced, step_size=step2decay, gamma=decay_lr)\n",
    "\n",
    "\n",
    "    iter_log = IterLog()\n",
    "    train_log = TrainLog()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        iter_log.on_iter_start()\n",
    "\n",
    "        rs_indiced.model.train() \n",
    "        rs_raw.model.train() \n",
    "\n",
    "\n",
    "        for inputs_raw, inputs_indiced, labels in train_loader:\n",
    "            inputs_raw, inputs_indiced, labels = inputs_raw.to(device, dtype=torch.float), inputs_indiced.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
    "\n",
    "            optimizer_raw.zero_grad()\n",
    "            optimizer_indiced.zero_grad()\n",
    "\n",
    "            outputs_raw = rs_raw.forward(inputs_raw)\n",
    "            outputs_indiced = rs_indiced.forward(inputs_indiced)\n",
    "\n",
    "            loss_raw = loss_fn_raw(outputs_raw, labels)\n",
    "            loss_raw.backward()\n",
    "\n",
    "            loss_indiced = loss_fn_indiced(outputs_indiced, labels)\n",
    "            loss_indiced.backward()\n",
    "\n",
    "            optimizer_raw.step()\n",
    "            optimizer_indiced.step()\n",
    "\n",
    "            scheduler_raw.step()\n",
    "            scheduler_indiced.step()\n",
    "\n",
    "            _, predicted_raw = torch.max(outputs_raw, 1)\n",
    "            _, predicted_indiced = torch.max(outputs_indiced, 1)\n",
    "\n",
    "            metric_raw = metric_fn(predicted_raw, labels)\n",
    "            metric_indiced = metric_fn(predicted_indiced, labels)\n",
    "\n",
    "            iter_log.add_on_train_iter_end(train_loss_indiced=loss_indiced.item(), train_loss_raw=loss_raw.item(),\n",
    "                                           train_metric_raw=metric_raw.item(), train_metric_indiced=metric_indiced.item())\n",
    "\n",
    "        rs_raw.model.eval()\n",
    "        rs_indiced.model.eval()\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs_raw, inputs_indiced, labels in val_loader:\n",
    "                inputs_raw, inputs_indiced, labels = inputs_raw.to(device, dtype=torch.float), inputs_indiced.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
    "\n",
    "                outputs_raw = rs_raw.forward(inputs_raw)\n",
    "                outputs_indiced = rs_indiced.forward(inputs_indiced)\n",
    "\n",
    "                loss_raw = loss_fn_raw(outputs_raw, labels)\n",
    "                loss_indiced = loss_fn_indiced(outputs_indiced, labels)\n",
    "\n",
    "                _, predicted_raw = torch.max(outputs_raw, 1)\n",
    "                _, predicted_indiced = torch.max(outputs_indiced, 1)\n",
    "                \n",
    "                metric_raw = metric_fn(predicted_raw, labels)\n",
    "                metric_indiced = metric_fn(predicted_indiced, labels)\n",
    "\n",
    "                iter_log.add_on_val_iter_end(val_loss_indiced=loss_indiced.item(), val_loss_raw=loss_raw.item(),\n",
    "                                             val_metric_raw=metric_raw.item(), val_metric_indiced=metric_indiced.item())\n",
    "\n",
    "        iter_log.on_epoch_end()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], {iter_log}\")\n",
    "\n",
    "        train_log.on_epoch_end(iter_log)\n",
    "\n",
    "    train_log.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, gt):\n",
    "    return (pred == gt).sum() / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_and_evaluate() got multiple values for argument 'step2decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep2decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_and_evaluate() got multiple values for argument 'step2decay'"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(train_dataloader_s2, val_dataloader_s2, 0.001, 0.0001, CrossEntropyLoss, accuracy, 'cuda:0', 40, 'test', step2decay=15, decay_lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_loader, val_loader, lr, weight_decay, loss_fn, metric_fn,\n",
    "                       device, num_epochs, experiment_name, scheduler):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
